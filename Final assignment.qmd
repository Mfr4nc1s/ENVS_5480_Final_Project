```{r}
##install packages and run libraries

install.packages("future")
install.packages("Rsagacmd")
install.packages("lidR")
install.packages("units")
library(units)
library(lidR)
library(future)
library(terra)
library(mapview)
library(bcdata)
library(bcmaps)
library(tidyverse)
library(Rsagacmd)
```

```{r}
##bring las file in

lasfile<- "LacDuBois.las"
# las <- readLAS(lasfile)
las_ctg <- readLAScatalog("lacDuBois.las")

## perform a retiling to allow for parralel processing and core assignment to
## make the process more efficient, exactly the same as in the seminar

n_cores <- availableCores()
plan(multisession, workers = n_cores)

opt_output_files(las_ctg) <- "01_retile/{XLEFT}_{YBOTTOM}"

##Chunk size and buffer size:

opt_chunk_size(las_ctg) <- 500
opt_chunk_buffer(las_ctg) <- 0

##Chunk alignment


opt_chunk_alignment(las_ctg) <- c(500, 500)
plot(las_ctg, chunk_pattern = TRUE)

##retile

ctg_tiled <- catalog_retile(las_ctg)
View(ctg_tiled)
plot(ctg_tiled, mapview = TRUE)

##Catalog options are carried over from one processed catalog to the next,
##so we need to reset the filter to ensure that we don't further filter
##the catalog:

opt_filter(ctg_tiled) <- ""

##Reset chunk size to 0 - we don't need to perform retiling here because
##that is done, so reset chunk size to 0 to prevent further retiling

opt_chunk_size(ctg_tiled) <- 0

##Set a 15m chunk buffer to load in 15m of the surrounding tiles

opt_chunk_buffer(ctg_tiled) <- 15

## Specify output path to be in a folder named "ground". The {*} is an indicator
## to keep the original file name, which will be the tiled file name:

opt_output_files(ctg_tiled) <- "02_ground/{*}"

##Now, do the ground point classification:

ctg_ground <- classify_ground(ctg_tiled, algorithm = csf(sloop_smooth = TRUE))
opt_output_files(ctg_ground) <- ""
dem <- rasterize_terrain(ctg_ground, res = 5, algorithm = tin())

##Create a new folder called "ta" to hold all of the files used for terrain
##analysis:


dir.create("ta/5m", showWarnings = FALSE)
dem <- writeRaster(dem, "ta/5m/dem.tif", overwrite = TRUE)
```

```{r}
library(terra)
library(mapview)


dem <- rast("DEMs/dem.tif")
plot(dem)


#mapview(dem)


## extract terrain layers

## reminder to change path back to microsoft
##
##saga_path <- "C:/Users/T00629284/OneDrive - Thompson Rivers University/Desktop/saga-9.3.2_x64/saga_cmd.exe"

saga_path <- ("/Applications/SAGA.app/Contents/MacOS/saga_cmd")

saga <- saga_gis(saga_path, raster_format = "GeoTIFF")
#dem = rast("ta/dem.tif")
 
dem_preproc <- saga$ta_preprocessor$sink_removal(
  dem = dem, 

  dem_preproc = "ta/5m/dem_preproc.tif",
  #dem_preproc = "ta/dem_preproc.tif"
  )

sources(dem_preproc)

plot(dem_preproc)

## determine slope & aspect
 
saga$ta_morphometry$slope_aspect_curvature
 
View(tidy(saga$ta_morphometry$slope_aspect_curvature))
 
slope_aspect <- saga$ta_morphometry$slope_aspect_curvature(
  elevation = dem_preproc, 

  slope = "ta/5m/slope.tif", 
  aspect = "ta/5m/aspect.tif", 
  method = 6, 
  unit_slope = "radians", 
  unit_aspect = "radians",
  .all_outputs = FALSE
  )

## determine MRVBF
 
mrvbf_thresh <- mrvbf_threshold(res = res(dem)[1])
mrvbf <- saga$ta_morphometry$multiresolution_index_of_valley_bottom_flatness_mrvbf(
  dem = dem_preproc, 

  mrvbf = "ta/5m/mrvbf.tif",
  mrrtf = "ta/5m/mrrtf.tif", 
  t_slope = mrvbf_thresh
  )

## determine TRI
 
tri <- saga$ta_morphometry$terrain_ruggedness_index_tri(

  dem = dem_preproc, tri = "ta/5m/tri.tif")

##  determine TWI
 
tca <- saga$ta_hydrology$flow_accumulation_top_down(

  elevation = dem_preproc, flow = "ta/5m/tca_TEMP.tif", .all_outputs = FALSE)
 
sca <- saga$ta_hydrology$flow_width_and_specific_catchment_area(
  dem = dem_preproc, tca = tca, sca = "ta/5m/sca_TEMP.tif", .all_outputs = FALSE)
 
twi <- saga$ta_hydrology$topographic_wetness_index(
  slope = slope_aspect$slope, area = sca, twi = "ta/5m/twi.tif")

## determine TPI

tpi <- saga$ta_morphometry$topographic_position_index_tpi(
  dem = dem_preproc, 

  tpi = "ta/5m/tpi.tif"
)

## determine topographic openess

# Topographic openness
openness <- saga$ta_lighting$topographic_openness(
  dem = dem_preproc, 
  pos = "ta/5m/openness_pos.tif", 
  neg = "ta/5m/openness_neg.tif"
  )

# Remove files
files_to_remove <- list.files("ta/5m", pattern = "_TEMP.tif", full.names = TRUE)
file.remove(files_to_remove)
saga_remove_tmpfiles()

```

Extraction & Modelling

```{r}

library(remotes)
library(tidyverse)
library(ranger) 
library(mlr3verse)
library(mlr3spatial)
library(mlr3spatiotempcv)
library(sf)
library(terra, exclude = "resample")
library(future)

lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")

```

```{r}

library(sf)
library(sfheaders)
library(dplyr)

##load raster data
rast_files <- list.files("5m", pattern = ".tif", full.names = TRUE)

# remove listed files that have "dem" in the name
rast_files <- grep("dem", rast_files, value = TRUE, invert = TRUE)

# load those files as SpatRaster objects
rasters <- rast(rast_files)
rasters
plot(rasters)

# Create outline of raster area
single_rast <- rasters[[1]]*0
rast_outline <- as.polygons(single_rast) %>%
  st_as_sf() %>% 
  sf_remove_holes() %>%
  st_geometry()

## load vector data & read in sf dataframe, clip it to the raster outline
ldb_sk_poly <- st_read("LDB_SK.gpkg") %>%
  st_set_agr("constant") %>%
  st_intersection(rast_outline)


# "absent" polygons overlap with "present" polygons. This will confuse the model.
# Clip the absent polygons using the present polygons to prevent that confusion.
ldb_sk_present <- ldb_sk_poly |> filter(sk_pres)
ldb_sk_absent <- ldb_sk_poly |> filter(!sk_pres)
ldb_present_union <- st_union(ldb_sk_present)
ldb_sk_absent_clip <- st_difference(ldb_sk_absent, ldb_present_union)
ldb_sk_poly <- rbind(ldb_sk_present, ldb_sk_absent_clip) |> 
  mutate(sk_pres = factor(sk_pres, levels = c(TRUE, FALSE)))

ldb_sk_poly

vectors <- vect(ldb_sk_poly)

# Perform data extraction

### Changing this to use the middle of the polygon instead of getting all 
### points in the polygon - all points was way too much data to work with
extraction <- terra::extract(
  rasters, vectors, fun = mean, na.rm = TRUE, bind = TRUE, ID = FALSE) %>% 
  st_as_sf() %>%
  na.omit() %>%
  st_set_agr("constant") %>%
  st_centroid(of_largest_polygon = TRUE)

#extraction_join = subset(extraction_join, select = -c(SITE_CREATED_DATE)) 
#Maybe this was just me but I had to remove the date column
#in order to run the task line below

```

Read files in\
points/polygons & raster imagery\
Question #6: extract value of raster at each point/line/polygon

```{r}
library(mlr3verse)
library(mlr3spatial)
library(mlr3spatiotempcv)


task_sk <- as_task_classif_st(extraction, target = "sk_pres", positive = "TRUE")
task_sk
##created task


```

Tuner design: Set up how the 'to_tune()' function will work

```{r}

df_design <- expand.grid(
  num.trees = c(100, 250, 500, 750, 1000, 1500, 2000),
  mtry = 1:length(task_sk$feature_names))

dt_design <- data.table(df_design)

tnr_design <- tnr("design_points", design = dt_design)

```

Cross Validation

```{r}
cv_inner <- rsmp("cv", folds = 10)
cv_outer <- rsmp("spcv_coords", folds = 4)

```

Parallelizing

```{r}

plan(multisession)

plan(list("sequential", "multisession"))

plan(list(
  tweak("multisession", workers = 2),
  tweak("multisession", workers = availableCores() / 2)
))

```

Cores

```{r}

outer_cores <- min(4, availableCores())
inner_cores <- floor(availableCores() / outer_cores)

plan(list(
  tweak("multisession", workers = outer_cores),
  tweak("multisession", workers = inner_cores)
))

```


Run Probability model

```{r}

# Probability learner
lrn_rf_tune_prob <- lrn("classif.ranger", 
                        num.trees = to_tune(100, 2000), predict_type = "prob",
                        mtry = to_tune(1, length(task_sk$feature_names)),
                        importance = "impurity")

avail_msrs <- as.data.table(msrs())[
  task_type == "classif" & 
    predict_type == "prob" &
    task_properties == "twoclass"] # it is twoclass: only TRUE/FALSE as response

View(avail_msrs)

#useful_msrs <- c("classif.logloss", "classif.mbrier")
useful_msrs <- c("classif.auc", "classif.bbrier")

# Create the auto_tuner object
at_prob <- auto_tuner(
  tuner = tnr_design,
  learner = lrn_rf_tune_prob,
  resampling = cv_inner,
  measure = msr("classif.auc"),
  terminator = trm("none")
)

rr_prob <- resample(task_sk, at_prob, cv_outer, store_models = TRUE)


# Aggregate the final data to generate an overall score:
rr_prob$aggregate(msrs(useful_msrs))
##   classif.auc classif.bbrier 
##     0.6459389      0.2436455 


# Aggregated confusion matrix:
conf_prob <- rr_prob$prediction()$confusion
conf_prob
##        truth
##response TRUE FALSE
##   TRUE   205   145
##   FALSE  134   198


# Get best learner
mod_scores_prob <- rr_prob$score(msrs(useful_msrs))
View(mod_scores_prob)

msr("classif.auc")     
msr("classif.bbrier")  

# Choosing model based on maximizing classif.auc
best_lrn_prob <- rr_prob$learners[[which.max(mod_scores_prob$classif.auc)]]$learner

# Variable importance of the best learner
imp <- best_lrn_prob$importance()
imp <- data.frame(Variable = factor(names(imp), levels = rev(unique(names(imp)))),
                  Importance = imp, row.names = NULL)

# Plot that importance:
imp_plot_prob <- ggplot(imp, aes(x = Importance, y = Variable)) + 
  geom_bar(stat = "identity")

imp_plot_prob
##All having importance scores above 20. All scores between 21 & 24, highest = TPI, lowest = slope


# Write variable importance to .csv file:
write.ftable(ftable(conf_prob), file = "confusion_matrix_prob.csv", sep = ",",
             quote = FALSE)

ggsave("Variable importance_prob.png", imp_plot_prob, width = 1920, height = 1440,
       units = "px", dpi = 300)



```

Probability Prediction

```{r}

# Set up parallel environment:
plan(multisession)
best_lrn_prob$parallel_predict <- TRUE

# Generate map prediction
prediction_prob <- predict_spatial(rasters, best_lrn_prob)
plot(prediction_prob)

# Workaround for probability prediction
ranger_model <- best_lrn_prob$model
fun <- function(model, ...) predict(model, ...)$predictions
prediction_prob_terra <- terra::predict(
  rasters, ranger_model, fun = fun, na.rm = TRUE)
plot(prediction_prob_terra)

# Check, sum to 1
check <- app(prediction_prob_terra, fun = sum, na.rm = TRUE, 
             cores = availableCores())

plot(check, type = "classes")
## answers from .99999 to 1.0000. Therefore all sum to 1

```

Shannon Entropy

```{r}

entropy_step1 <- sapp(prediction_prob_terra, function(x) x * log(x))
entropy_step2 <- -app(entropy_step1, fun = sum, cores = availableCores())
plot(entropy_step2)

```

Writing Outputs

```{r}

prob_files <- paste0(names(prediction_prob_terra), "_probability.tif")
prediction_prob_terra <- writeRaster(
  prediction_prob_terra, prob_files, overwrite = TRUE)
##saves a vector file of each as .tif

# Also, write entropy layer:
entropy_step2 <- writeRaster(entropy_step2, "entropy.tif", overwrite = TRUE)

```
