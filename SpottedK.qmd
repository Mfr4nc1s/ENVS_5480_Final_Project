---
title: "SpottedK"
format: html
editor: visual
---

```{r}
##add to Nicole branch
```

#6

```{r}

library(remotes)
library(mlr3spatiotempcv)
library(tidyverse)
library(ranger) 
library(mlr3verse)
library(mlr3spatial)
library(mlr3spatiotempcv)
library(sf)
library(terra, exclude = "resample")
library(future)

lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")

```

Read files in\
points/polygons & raster imagery\
Question #5: extract value of raster at each point/line/polygon

```{r}
##load raster data
rast_data <- system.file("extdata", "editraster.tif", package = "mlr3spatial")
edit_raster <- rast(rast_data)
edit_raster
plot(edit_raster)

##load vector data & read in sf dataframe 
vect_data <-system.file("extdata", "edit_points.gpkg", package = "mlr3spatial")
edit_vector <- st_read(vect_data, stringsAsFactors = TRUE)
edit_vector

##land_cover is what is predicted....change for our project
tsk_edit <- as_task_classif_st(edit_vector, target = "land_cover??")
tsk_edit
##created task

```

Simple random forest learner

```{r}
lrn_rf <- lrn("classif.ranger", num.trees = 500, mtry = 2) ##edit mtry
lrn_rf


```

Train the learner using task data

```{r}

lrn_rf_tune <- lrn("classif.ranger", 
                   num.trees = to_tune(100, 2000), predict_type = "response??",
                   mtry = to_tune(1, length(tsk_edit$feature_names??)),
                   importance = "impurity")

```

Tuner design: Set up how the 'to_tune()' function will work

```{r}

df_design <- expand.grid(
  num.trees = c(100, 250, 500, 750, 1000, 1500, 2000),
  mtry = 1:length(tsk_edit$feature_names))

dt_design <- data.table(df_design)

tnr_design <- tnr("design_points", design = dt_design)


```

Cross Validation

```{r}
cv_inner <- rsmp("cv", folds = 10)
cv_outer <- rsmp("spcv_coords", folds = 4)

##autotuner object
at <- auto_tuner(
  tuner = tnr_design,
  learner = lrn_rf_tune,
  resampling = cv_inner,
  measure = msr("classif.ce"),
  terminator = trm("none")
)

```

Parallelizing

```{r}

plan(multisession)

plan(list("sequential", "multisession"))

plan(list(
  tweak("multisession", workers = 2),
  tweak("multisession", workers = availableCores() / 2)
))

```

Cores

```{r}

outer_cores <- min(4, availableCores())
inner_cores <- floor(availableCores() / outer_cores)

plan(list(
  tweak("multisession", workers = outer_cores),
  tweak("multisession", workers = inner_cores)
))


```

Run Probability model

```{r}

# Probability learner
lrn_rf_tune_prob <- lrn("classif.ranger", 
                        num.trees = to_tune(100, 2000), predict_type = "prob",
                        mtry = to_tune(1, length(tsk_edit$feature_names)),
                        importance = "impurity")

##??
avail_msrs <- as.data.table(msrs())[
  task_type == "classif" & 
    predict_type == "prob" &
    task_properties != "twoclass"]

View(avail_msrs)

useful_msrs <- c("classif.logloss", "classif.mbrier")

# Create the auto_tuner object
at_prob <- auto_tuner(
  tuner = tnr_design,
  learner = lrn_rf_tune_prob,
  resampling = cv_inner,
  measure = msr("classif.logloss"),
  terminator = trm("evals", n_evals = 5)
)

rr_prob <- resample(tsk_leipzig, at_prob, cv_outer, store_models = TRUE)

# Aggregate the final data to generate an overall score:
rr_prob$aggregate(msrs(useful_msrs))

# Aggregated confusion matrix:
conf_prob <- rr_prob$prediction()$confusion
conf_prob

# Get best learner
mod_scores_prob <- rr_prob$score(msrs(useful_msrs))
View(mod_scores_prob)

##what to minimize or maximize - want to maximixe accuracy (says true or false)
msr("classif.logloss")
msr("classif.mbrier")

# Choosing model based on lowest log loss values:
best_lrn_prob <- rr_prob$learners[[which.min(mod_scores_prob$classif.logloss)]]$learner

# Variable importance of the best learner
imp <- best_lrn_prob$importance()
imp <- data.frame(Variable = factor(names(imp), levels = rev(unique(names(imp)))),
                  Importance = imp, row.names = NULL)

# Plot that importance:
imp_plot_prob <- ggplot(imp, aes(x = Importance, y = Variable)) + 
  geom_bar(stat = "identity")

imp_plot_prob

# Write variable importance to .csv file:
write.ftable(ftable(conf_prob), file = "confusion_matrix_prob.csv", sep = ",",
             quote = FALSE)

ggsave("Variable importance_prob.png", imp_plot_prob, width = 1920, height = 1440,
       units = "px", dpi = 300)



```

Probability Prediction

```{r}

# Set up parallel environment:
plan(multisession)
best_lrn_prob$parallel_predict <- TRUE

# Generate map prediction
prediction_prob <- predict_spatial(leipzig_raster, best_lrn_prob)
plot(prediction_prob)

# Workaround for probability prediction
ranger_model <- best_lrn_prob$model
fun <- function(model, ...) predict(model, ...)$predictions
prediction_prob_terra <- terra::predict(
  leipzig_raster, ranger_model, fun = fun, na.rm = TRUE)

plot(prediction_prob_terra)

# Check, sum to 1
check <- app(prediction_prob_terra, fun = sum, na.rm = TRUE, 
             cores = availableCores())

plot(check, type = "classes")


```

Shannon Entropy

```{r}

entropy_step1 <- sapp(prediction_prob_terra, function(x) x * log(x))
entropy_step2 <- -app(entropy_step1, fun = sum, cores = availableCores())
plot(entropy_step2)

```

Map Predictions

Raster image of predictions

```{r}

# Set up parallel environment:
plan(multisession)
best_lrn_regr$parallel_predict <- TRUE

# Generate map prediction
prediction_regr <- predict_spatial(leipzig_raster, best_lrn_regr)
plot(prediction_regr)


```

Writing Outputs

```{r}

prob_files <- paste0(names(prediction_prob_terra), "_probability.tif")
prediction_prob_terra <- writeRaster(
  prediction_prob_terra, prob_files, overwrite = TRUE)
##saves a vector file of each as .tif

# Also, write entropy layer:
entropy_step2 <- writeRaster(entropy_step2, "entropy.tif", overwrite = TRUE)

```
